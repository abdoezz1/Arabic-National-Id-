{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VT73e8fNRQfV",
        "outputId": "da976b7c-4204-4a1b-b948-757ef313ae33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.171)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.16.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.12.0.88)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.3.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.1.1)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.11.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr\n",
        "import cv2\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class SimplifiedArabicIDDetector:\n",
        "    def __init__(self, main_model_path: str, digits_model_path: str,enable_manual_correction: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the simplified two-stage detector\n",
        "\n",
        "        Args:\n",
        "            main_model_path: Path to the main YOLO model (detects fields + ID card)\n",
        "            digits_model_path: Path to the digits-only YOLO model\n",
        "        \"\"\"\n",
        "        self.main_model = YOLO(main_model_path)\n",
        "        self.digits_model = YOLO(digits_model_path)\n",
        "        self.reader = easyocr.Reader(['ar'], gpu=True)\n",
        "        self.enable_manual_correction = enable_manual_correction\n",
        "\n",
        "        self.confidence_thresholds = {\n",
        "            'national_id': 0.3,\n",
        "            'first_name': 0.4,\n",
        "            'last_name': 0.4,\n",
        "            'address1': 0.3,\n",
        "            'address2': 0.3,\n",
        "            'id': 0.5,\n",
        "            'digit': 0.3\n",
        "        }\n",
        "\n",
        "        self.digit_class_mapping = self._initialize_digit_mapping()\n",
        "        self.expected_digits = 14\n",
        "\n",
        "    def _initialize_digit_mapping(self) -> Dict[int, str]:\n",
        "        \"\"\"Initialize mapping from class IDs to digit values\"\"\"\n",
        "        digit_mapping = {}\n",
        "        for class_id, class_name in self.digits_model.names.items():\n",
        "            if class_name.isdigit():\n",
        "                digit_mapping[class_id] = class_name\n",
        "        return digit_mapping\n",
        "\n",
        "    def calculate_circular_std(self, angles):\n",
        "        \"\"\"Calculate circular standard deviation for angles\"\"\"\n",
        "        angles = np.array(angles)\n",
        "        angles_rad = np.radians(angles * 2)\n",
        "        mean_cos = np.mean(np.cos(angles_rad))\n",
        "        mean_sin = np.mean(np.sin(angles_rad))\n",
        "        R = np.sqrt(mean_cos**2 + mean_sin**2)\n",
        "\n",
        "        if R < 1e-10:\n",
        "            circular_std = 180.0\n",
        "        else:\n",
        "            circular_std = np.degrees(np.sqrt(-2 * np.log(R))) / 2\n",
        "        return circular_std\n",
        "\n",
        "    def check_if_rotation_needed(self, image: cv2.Mat) -> Tuple[bool, float]:\n",
        "        \"\"\"Check if rotation is needed and return rotation angle\"\"\"\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "        h, w = gray.shape\n",
        "        edges = cv2.Canny(gray, 30, 100, apertureSize=3)\n",
        "        min_votes = max(30, min(w, h) // 6)\n",
        "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=min_votes)\n",
        "\n",
        "        if lines is None or len(lines) < 2:\n",
        "            return False, 0.0\n",
        "\n",
        "        raw_angles = []\n",
        "        for rho, theta in lines[:min(8, len(lines)), 0]:\n",
        "            angle_deg = np.degrees(theta)\n",
        "            if angle_deg > 90:\n",
        "                angle_deg -= 180\n",
        "            raw_angles.append(angle_deg)\n",
        "\n",
        "        if not raw_angles:\n",
        "            return False, 0.0\n",
        "\n",
        "        rotation_angle, _ = self.determine_rotation_direction_and_angle(raw_angles)\n",
        "        circular_std = self.calculate_circular_std(raw_angles)\n",
        "        abs_rotation = abs(rotation_angle)\n",
        "\n",
        "        # Border validation for small rotations\n",
        "        if abs_rotation > 0.3 and abs_rotation < 15.0:\n",
        "            border_width = min(h, w) // 12\n",
        "            mask = np.zeros_like(gray)\n",
        "            mask[:border_width, :] = 255\n",
        "            mask[-border_width:, :] = 255\n",
        "            mask[:, :border_width] = 255\n",
        "            mask[:, -border_width:] = 255\n",
        "\n",
        "            masked_edges = cv2.bitwise_and(edges, mask)\n",
        "            min_votes_border = max(20, min(h, w) // 10)\n",
        "            border_lines = cv2.HoughLines(masked_edges, 1, np.pi/180, threshold=min_votes_border)\n",
        "\n",
        "            if border_lines is not None and len(border_lines) >= 3:\n",
        "                border_angles = []\n",
        "                for rho, theta in border_lines[:8, 0]:\n",
        "                    angle_deg = np.degrees(theta)\n",
        "                    if angle_deg > 90:\n",
        "                        angle_deg -= 180\n",
        "                    border_angles.append(angle_deg)\n",
        "\n",
        "                horizontal_borders = [a for a in border_angles if abs(a) <= 15]\n",
        "                vertical_borders = [a for a in border_angles if abs(abs(a) - 90) <= 15]\n",
        "\n",
        "                if len(horizontal_borders) >= 1 and len(vertical_borders) >= 1:\n",
        "                    h_median = np.median(horizontal_borders) if horizontal_borders else 0\n",
        "                    v_median = np.median(vertical_borders) if vertical_borders else 90\n",
        "                    border_rotation_h = abs(h_median)\n",
        "                    border_rotation_v = abs(90 - abs(v_median))\n",
        "                    max_border_rotation = max(border_rotation_h, border_rotation_v)\n",
        "\n",
        "                    if max_border_rotation < 1.0:\n",
        "                        return False, 0.0\n",
        "                    elif max_border_rotation * 3 < abs_rotation:\n",
        "                        return False, 0.0\n",
        "\n",
        "        # Determine if rotation is needed based on angle magnitude and scatter\n",
        "        if abs_rotation >= 80:\n",
        "            return True, rotation_angle\n",
        "        elif abs_rotation >= 5.0:\n",
        "            if circular_std > 60.0:\n",
        "                return False, rotation_angle\n",
        "            else:\n",
        "                return True, rotation_angle\n",
        "        elif abs_rotation >= 1.5:\n",
        "            if circular_std > 100.0:\n",
        "                return False, rotation_angle\n",
        "            else:\n",
        "                return True, rotation_angle\n",
        "        elif abs_rotation >= 0.3:\n",
        "            if circular_std > 10.0:\n",
        "                return False, rotation_angle\n",
        "            else:\n",
        "                return True, rotation_angle\n",
        "        else:\n",
        "            return False, rotation_angle\n",
        "\n",
        "    def determine_rotation_direction_and_angle(self, angles):\n",
        "        \"\"\"Determine the best rotation direction and angle for ID card alignment\"\"\"\n",
        "        angles = np.array(angles)\n",
        "        horizontal_angles = []\n",
        "        vertical_angles = []\n",
        "        diagonal_angles = []\n",
        "\n",
        "        for angle in angles:\n",
        "            normalized = angle\n",
        "            if normalized > 90:\n",
        "                normalized -= 180\n",
        "            elif normalized < -90:\n",
        "                normalized += 180\n",
        "\n",
        "            if abs(normalized) <= 15:\n",
        "                horizontal_angles.append(normalized)\n",
        "            elif abs(normalized) >= 75:\n",
        "                vertical_angles.append(normalized)\n",
        "            elif abs(abs(normalized) - 45) <= 15:\n",
        "                diagonal_angles.append(normalized)\n",
        "            else:\n",
        "                horizontal_angles.append(normalized)\n",
        "\n",
        "        h_median = np.median(horizontal_angles) if horizontal_angles else None\n",
        "        v_median = np.median(vertical_angles) if vertical_angles else None\n",
        "        d_median = np.median(diagonal_angles) if diagonal_angles else None\n",
        "\n",
        "        rotation_needed = 0.0\n",
        "        direction = \"no rotation needed\"\n",
        "\n",
        "        # Detect 90° rotation based on diagonal dominance\n",
        "        total_lines = len(horizontal_angles) + len(vertical_angles) + len(diagonal_angles)\n",
        "        diagonal_ratio = len(diagonal_angles) / total_lines if total_lines > 0 else 0\n",
        "\n",
        "        is_90_degree_rotation = (\n",
        "            (diagonal_ratio > 0.7 and len(diagonal_angles) >= 5) or\n",
        "            (len(diagonal_angles) >= 8 and len(diagonal_angles) > 2 * (len(horizontal_angles) + len(vertical_angles)))\n",
        "        )\n",
        "\n",
        "        if is_90_degree_rotation:\n",
        "            if d_median is not None:\n",
        "                if d_median < -30:\n",
        "                    rotation_needed = 90.0\n",
        "                    direction = \"clockwise 90.0°\"\n",
        "                elif d_median > 30:\n",
        "                    rotation_needed = -90.0\n",
        "                    direction = \"counter-clockwise 90.0°\"\n",
        "                else:\n",
        "                    rotation_needed = 90.0\n",
        "                    direction = \"clockwise 90.0°\"\n",
        "        elif len(horizontal_angles) > len(vertical_angles) and len(horizontal_angles) >= 3:\n",
        "            if h_median is not None and abs(h_median) < 15:\n",
        "                rotation_needed = 90.0\n",
        "                direction = \"clockwise 90.0°\"\n",
        "            elif h_median is not None:\n",
        "                rotation_needed = -h_median\n",
        "                if rotation_needed > 0:\n",
        "                    direction = f\"clockwise {abs(rotation_needed):.2f}°\"\n",
        "                elif rotation_needed < 0:\n",
        "                    direction = f\"counter-clockwise {abs(rotation_needed):.2f}°\"\n",
        "        elif len(vertical_angles) > 0 and v_median is not None:\n",
        "            if v_median > 45:\n",
        "                rotation_needed = 90.0 - v_median\n",
        "            elif v_median < -45:\n",
        "                rotation_needed = -90.0 - v_median\n",
        "            else:\n",
        "                rotation_needed = -v_median\n",
        "\n",
        "            if len(horizontal_angles) > 0 and len(vertical_angles) > 0:\n",
        "                if abs(rotation_needed) < 2.0:\n",
        "                    rotation_needed = 0.0\n",
        "                    direction = \"no rotation needed\"\n",
        "\n",
        "            if abs(rotation_needed) > 30:\n",
        "                rotation_needed = 0.0\n",
        "                direction = \"no rotation needed\"\n",
        "            elif abs(rotation_needed) < 0.5:\n",
        "                rotation_needed = 0.0\n",
        "                direction = \"no rotation needed\"\n",
        "            elif rotation_needed != 0.0:\n",
        "                if rotation_needed > 0:\n",
        "                    direction = f\"clockwise {abs(rotation_needed):.2f}°\"\n",
        "                elif rotation_needed < 0:\n",
        "                    direction = f\"counter-clockwise {abs(rotation_needed):.2f}°\"\n",
        "        elif len(horizontal_angles) > 0 and h_median is not None:\n",
        "            rotation_needed = -h_median\n",
        "            if rotation_needed > 0:\n",
        "                direction = f\"clockwise {abs(rotation_needed):.2f}°\"\n",
        "            elif rotation_needed < 0:\n",
        "                direction = f\"counter-clockwise {abs(rotation_needed):.2f}°\"\n",
        "\n",
        "        return rotation_needed, direction\n",
        "\n",
        "    def detect_rotation_angle_advanced(self, image: cv2.Mat) -> float:\n",
        "        \"\"\"Advanced rotation detection for small rotations\"\"\"\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "        h, w = gray.shape\n",
        "        edges = cv2.Canny(gray, 30, 120)\n",
        "\n",
        "        # Border edge detection\n",
        "        mask = np.zeros_like(gray)\n",
        "        border_width = min(h, w) // 12\n",
        "        mask[:border_width, :] = 255\n",
        "        mask[-border_width:, :] = 255\n",
        "        mask[:, :border_width] = 255\n",
        "        mask[:, -border_width:] = 255\n",
        "\n",
        "        masked_edges = cv2.bitwise_and(edges, mask)\n",
        "        min_votes = max(25, min(h, w) // 8)\n",
        "        lines = cv2.HoughLines(masked_edges, 1, np.pi/180, threshold=min_votes)\n",
        "\n",
        "        if lines is not None and len(lines) >= 2:\n",
        "            angles = []\n",
        "            for rho, theta in lines[:10, 0]:\n",
        "                angle_deg = np.degrees(theta)\n",
        "                if angle_deg > 90:\n",
        "                    angle_deg -= 180\n",
        "                angles.append(angle_deg)\n",
        "\n",
        "            if angles:\n",
        "                angles = np.array(angles)\n",
        "                if len(angles) > 3:\n",
        "                    mean_angle = np.mean(angles)\n",
        "                    std_angle = np.std(angles)\n",
        "                    if std_angle > 0:\n",
        "                        filtered_angles = angles[abs(angles - mean_angle) <= 2 * std_angle]\n",
        "                        if len(filtered_angles) > 0:\n",
        "                            angles = filtered_angles\n",
        "\n",
        "                return float(np.median(angles))\n",
        "\n",
        "        # Fallback to general line detection\n",
        "        min_votes_general = max(20, min(h, w) // 10)\n",
        "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=min_votes_general)\n",
        "        if lines is not None and len(lines) >= 2:\n",
        "            angles = []\n",
        "            for rho, theta in lines[:8, 0]:\n",
        "                angle_deg = np.degrees(theta)\n",
        "                if angle_deg > 90:\n",
        "                    angle_deg -= 180\n",
        "                angles.append(angle_deg)\n",
        "\n",
        "            if angles:\n",
        "                angles = np.array(angles)\n",
        "                if len(angles) > 2:\n",
        "                    q75, q25 = np.percentile(angles, [75, 25])\n",
        "                    iqr = q75 - q25\n",
        "                    lower_bound = q25 - 1.5 * iqr\n",
        "                    upper_bound = q75 + 1.5 * iqr\n",
        "                    angles = angles[(angles >= lower_bound) & (angles <= upper_bound)]\n",
        "\n",
        "                if len(angles) > 0:\n",
        "                    return float(np.median(angles))\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def rotate_image(self, image: cv2.Mat, angle: float) -> cv2.Mat:\n",
        "        \"\"\"Rotate image by given angle\"\"\"\n",
        "        if abs(angle) < 0.5:\n",
        "            return image\n",
        "\n",
        "        h, w = image.shape[:2]\n",
        "        center = (w // 2, h // 2)\n",
        "        rotation_matrix = cv2.getRotationMatrix2D(center, -angle, 1.0)\n",
        "\n",
        "        cos_val = abs(rotation_matrix[0, 0])\n",
        "        sin_val = abs(rotation_matrix[0, 1])\n",
        "        new_w = int((h * sin_val) + (w * cos_val))\n",
        "        new_h = int((h * cos_val) + (w * sin_val))\n",
        "\n",
        "        rotation_matrix[0, 2] += (new_w / 2) - center[0]\n",
        "        rotation_matrix[1, 2] += (new_h / 2) - center[1]\n",
        "\n",
        "        rotated = cv2.warpAffine(image, rotation_matrix, (new_w, new_h),\n",
        "                                borderMode=cv2.BORDER_CONSTANT,\n",
        "                                borderValue=(255, 255, 255))\n",
        "        return rotated\n",
        "\n",
        "    def correct_rotation_smart(self, image: cv2.Mat) -> Tuple[cv2.Mat, float]:\n",
        "        \"\"\"Detect and correct image rotation\"\"\"\n",
        "        needs_rotation, detected_angle = self.check_if_rotation_needed(image)\n",
        "\n",
        "        if not needs_rotation:\n",
        "            return image, 0.0\n",
        "\n",
        "        precise_angle = self.detect_rotation_angle_advanced(image)\n",
        "\n",
        "        if abs(precise_angle) > 0.1 and abs(precise_angle) < 15.0:\n",
        "            final_angle = precise_angle\n",
        "        else:\n",
        "            final_angle = detected_angle\n",
        "\n",
        "        if abs(final_angle) > 0.1:\n",
        "            corrected_image = self.rotate_image(image, final_angle)\n",
        "            return corrected_image, final_angle\n",
        "        else:\n",
        "            return image, 0.0\n",
        "\n",
        "    def enhance_text_darkness(self, image: cv2.Mat) -> cv2.Mat:\n",
        "        \"\"\"Darken text regions for better digit detection\"\"\"\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image\n",
        "        text_mask = cv2.adaptiveThreshold(\n",
        "            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY_INV, 15, 10\n",
        "        )\n",
        "        text_mask_color = cv2.merge([text_mask] * 3)\n",
        "        darkened_image = image.copy()\n",
        "        darkened_image[text_mask_color == 255] = (darkened_image[text_mask_color == 255] * 0.2).astype(np.uint8)\n",
        "        return darkened_image\n",
        "\n",
        "    def crop_id_card(self, image_path: str) -> Tuple[str, cv2.Mat, cv2.Mat, Dict]:\n",
        "        \"\"\"Stage 1: Use main model to detect and crop the ID card\"\"\"\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Could not load image: {image_path}\")\n",
        "\n",
        "        results = self.main_model(image_path)[0]\n",
        "\n",
        "        try:\n",
        "            card_class_id = [k for k, v in self.main_model.names.items() if v == 'id'][0]\n",
        "        except IndexError:\n",
        "            raise ValueError(\"'id' class not found in main model!\")\n",
        "\n",
        "        card_boxes = results.boxes.xyxy.cpu().numpy()\n",
        "        class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
        "        confidences = results.boxes.conf.cpu().numpy()\n",
        "\n",
        "        id_boxes = [(box, conf) for box, cls, conf in zip(card_boxes, class_ids, confidences)\n",
        "                   if cls == card_class_id and conf >= self.confidence_thresholds['id']]\n",
        "\n",
        "        if not id_boxes:\n",
        "            raise ValueError(\"No ID card detected!\")\n",
        "\n",
        "        best_box, best_conf = max(id_boxes, key=lambda x: x[1])\n",
        "        x1, y1, x2, y2 = map(int, best_box)\n",
        "        cropped = image[y1:y2, x1:x2]\n",
        "\n",
        "        # Save original cropped image\n",
        "        original_cropped_path = image_path.replace('.jpg', '_cropped_original.jpg').replace('.png', '_cropped_original.png')\n",
        "        cv2.imwrite(original_cropped_path, cropped)\n",
        "\n",
        "        # Apply rotation correction\n",
        "        corrected_cropped, rotation_angle = self.correct_rotation_smart(cropped)\n",
        "\n",
        "        # Save rotation-corrected image if rotation was applied\n",
        "        rotation_corrected_path = None\n",
        "        if abs(rotation_angle) > 0.1:\n",
        "            rotation_corrected_path = image_path.replace('.jpg', '_rotation_corrected.jpg').replace('.png', '_rotation_corrected.png')\n",
        "            cv2.imwrite(rotation_corrected_path, corrected_cropped)\n",
        "\n",
        "        # Apply text enhancement for digit detection\n",
        "        enhanced_cropped = self.enhance_text_darkness(corrected_cropped)\n",
        "\n",
        "        # Save final processed image\n",
        "        cropped_path = image_path.replace('.jpg', '_cropped_id.jpg').replace('.png', '_cropped_id.png')\n",
        "        cv2.imwrite(cropped_path, enhanced_cropped)\n",
        "\n",
        "        detection_info = {\n",
        "            'bbox': (x1, y1, x2, y2),\n",
        "            'confidence': float(best_conf),\n",
        "            'cropped_path': cropped_path,\n",
        "            'original_cropped_path': original_cropped_path,\n",
        "            'rotation_corrected_path': rotation_corrected_path,\n",
        "            'rotation_corrected': abs(rotation_angle) > 0.1,\n",
        "            'rotation_angle': float(rotation_angle)\n",
        "        }\n",
        "\n",
        "        return cropped_path, corrected_cropped, enhanced_cropped, detection_info\n",
        "\n",
        "    def detect_digits_yolo_only(self, cropped_id_path: str) -> List[Dict]:\n",
        "        \"\"\"Stage 2: Detect exactly 14 digits using YOLO only\"\"\"\n",
        "        results = self.digits_model(cropped_id_path)[0]\n",
        "\n",
        "        if len(results.boxes) == 0:\n",
        "            return []\n",
        "\n",
        "        boxes = results.boxes.xyxy.cpu().numpy()\n",
        "        confidences = results.boxes.conf.cpu().numpy()\n",
        "        class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
        "\n",
        "        digit_detections = []\n",
        "        for box, conf, cls_id in zip(boxes, confidences, class_ids):\n",
        "            if conf >= self.confidence_thresholds['digit']:\n",
        "                digit_value = self.digit_class_mapping.get(cls_id, None)\n",
        "                if digit_value is not None:\n",
        "                    x1, y1, x2, y2 = map(int, box)\n",
        "                    digit_detections.append({\n",
        "                        'bbox': (x1, y1, x2, y2),\n",
        "                        'confidence': float(conf),\n",
        "                        'class_id': int(cls_id),\n",
        "                        'digit_value': digit_value,\n",
        "                        'x_center': (x1 + x2) / 2,\n",
        "                        'y_center': (y1 + y2) / 2\n",
        "                    })\n",
        "\n",
        "        # Sort by position (left to right)\n",
        "        digit_detections.sort(key=lambda x: x['x_center'])\n",
        "\n",
        "        # Keep top 14 digits by confidence if more than 14 detected\n",
        "        if len(digit_detections) > self.expected_digits:\n",
        "            sorted_by_conf = sorted(digit_detections, key=lambda x: x['confidence'], reverse=True)\n",
        "            top_digits = sorted_by_conf[:self.expected_digits]\n",
        "            digit_detections = sorted(top_digits, key=lambda x: x['x_center'])\n",
        "\n",
        "        return digit_detections\n",
        "\n",
        "    def construct_national_id_simple(self, digit_detections: List[Dict]) -> Tuple[str, Dict]:\n",
        "        \"\"\"Construct 14-digit national ID from YOLO detections\"\"\"\n",
        "        if not digit_detections:\n",
        "            return \"\", {\"method\": \"yolo_only\", \"status\": \"failed\", \"reason\": \"no_detections\"}\n",
        "\n",
        "        national_id = ''.join([d['digit_value'] for d in digit_detections])\n",
        "\n",
        "        result_info = {\n",
        "            \"method\": \"yolo_only\",\n",
        "            \"status\": \"success\" if len(national_id) == self.expected_digits else \"partial\",\n",
        "            \"detected_digits\": len(digit_detections),\n",
        "            \"expected_digits\": self.expected_digits,\n",
        "            \"confidence_scores\": [d['confidence'] for d in digit_detections],\n",
        "            \"avg_confidence\": float(np.mean([d['confidence'] for d in digit_detections])),\n",
        "            \"min_confidence\": float(np.min([d['confidence'] for d in digit_detections])),\n",
        "            \"max_confidence\": float(np.max([d['confidence'] for d in digit_detections]))\n",
        "        }\n",
        "\n",
        "        return national_id, result_info\n",
        "\n",
        "    def preprocess_for_text(self, image: cv2.Mat) -> cv2.Mat:\n",
        "        \"\"\"Standard preprocessing for Arabic text\"\"\"\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        denoised = cv2.fastNlMeansDenoising(gray, h=10)\n",
        "        enhanced = cv2.convertScaleAbs(denoised, alpha=1.3, beta=15)\n",
        "        return enhanced\n",
        "\n",
        "\n",
        "    def manual_correction_interface(self, fields: Dict[str, str]) -> Dict[str, str]:\n",
        "        \"\"\"Interactive interface for manual corrections\"\"\"\n",
        "        if not self.enable_manual_correction:\n",
        "            return fields\n",
        "\n",
        "        corrected_fields = fields.copy()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MANUAL CORRECTION MODE\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"Review and correct any OCR mistakes:\")\n",
        "        print(\"Press Enter to keep current value, or type new value to change\")\n",
        "        print(\"-\"*60)\n",
        "\n",
        "        field_names = {\n",
        "            'national_id': 'National ID',\n",
        "            'first_name': 'First Name',\n",
        "            'last_name': 'Last Name',\n",
        "            'address1': 'Address Line 1',\n",
        "            'address2': 'Address Line 2'\n",
        "        }\n",
        "\n",
        "        for field_key, field_display in field_names.items():\n",
        "            if field_key in corrected_fields and field_key != '_extraction_info':\n",
        "                current_value = corrected_fields[field_key]\n",
        "\n",
        "                if current_value:\n",
        "                    print(f\"\\n{field_display}:\")\n",
        "                    print(f\"Current: {current_value}\")\n",
        "\n",
        "                    # Show suggestions for common corrections\n",
        "                    # suggestions = self._get_correction_suggestions(current_value, field_key)\n",
        "                    # if suggestions:\n",
        "                    #     print(\"Suggestions:\")\n",
        "                    #     for i, suggestion in enumerate(suggestions[:3], 1):\n",
        "                    #         print(f\"  {i}. {suggestion}\")\n",
        "                    #     print(f\"  Or type your own correction:\")\n",
        "\n",
        "                    user_input = input(f\"New value (or Enter to keep): \").strip()\n",
        "\n",
        "                    if user_input:\n",
        "                        # # Check if user selected a suggestion number\n",
        "                        # if user_input.isdigit() and suggestions:\n",
        "                        #     suggestion_idx = int(user_input) - 1\n",
        "                        #     if 0 <= suggestion_idx < len(suggestions):\n",
        "                        #         corrected_fields[field_key] = suggestions[suggestion_idx]\n",
        "                        #         print(f\"✅ Changed to: {suggestions[suggestion_idx]}\")\n",
        "                        #     else:\n",
        "                        #         print(\"❌ Invalid suggestion number\")\n",
        "\n",
        "                        corrected_fields[field_key] = user_input\n",
        "                        print(f\"✅ Changed to: {user_input}\")\n",
        "                    else:\n",
        "                        print(\"✅ Kept original value\")\n",
        "                else:\n",
        "                    print(f\"\\n{field_display}: (Not detected)\")\n",
        "                    user_input = input(f\"Enter value manually (or Enter to skip): \").strip()\n",
        "                    if user_input:\n",
        "                        corrected_fields[field_key] = user_input\n",
        "                        print(f\"✅ Added: {user_input}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CORRECTION COMPLETED\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        return corrected_fields\n",
        "\n",
        "    def extract_text_from_roi(self, image: cv2.Mat, bbox: Tuple[int, int, int, int], field_type: str) -> str:\n",
        "        \"\"\"Extract text from ROI for non-digit fields\"\"\"\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        padding = 5\n",
        "        h, w = image.shape[:2]\n",
        "        x1 = max(0, x1 - padding)\n",
        "        y1 = max(0, y1 - padding)\n",
        "        x2 = min(w, x2 + padding)\n",
        "        y2 = min(h, y2 + padding)\n",
        "\n",
        "        roi = image[y1:y2, x1:x2]\n",
        "        if roi.size == 0:\n",
        "            return \"\"\n",
        "\n",
        "        processed_roi = self.preprocess_for_text(roi)\n",
        "        results = self.reader.readtext(processed_roi, detail=1, paragraph=True)\n",
        "\n",
        "        confidence_threshold = self.confidence_thresholds.get(field_type, 0.4)\n",
        "        best_text = \"\"\n",
        "\n",
        "        for result in results:\n",
        "            try:\n",
        "                if len(result) == 3:\n",
        "                    _, text, confidence = result\n",
        "                elif len(result) == 2:\n",
        "                    _, text = result\n",
        "                    confidence = 1.0\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if confidence >= confidence_threshold:\n",
        "                    cleaned_text = text.strip()\n",
        "                    if len(cleaned_text) > len(best_text):\n",
        "                        best_text = cleaned_text\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        return best_text\n",
        "\n",
        "    def detect_other_fields_from_processed_image(self, text_extraction_image: cv2.Mat, cropped_id_path: str) -> Dict[str, str]:\n",
        "        \"\"\"Extract other fields using the main model\"\"\"\n",
        "        results = self.main_model(cropped_id_path)[0]\n",
        "\n",
        "        boxes = results.boxes.xyxy.cpu().numpy()\n",
        "        class_ids = results.boxes.cls.cpu().numpy().astype(int)\n",
        "        confidences = results.boxes.conf.cpu().numpy()\n",
        "        class_names = self.main_model.names\n",
        "\n",
        "        detected_fields = {\n",
        "            'first_name': '',\n",
        "            'last_name': '',\n",
        "            'address1': '',\n",
        "            'address2': ''\n",
        "        }\n",
        "\n",
        "        for box, class_id, conf in zip(boxes, class_ids, confidences):\n",
        "            field_type = class_names[class_id]\n",
        "            if field_type in detected_fields and conf >= self.confidence_thresholds.get(field_type, 0.4):\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "                extracted_text = self.extract_text_from_roi(text_extraction_image, (x1, y1, x2, y2), field_type)\n",
        "                if extracted_text:\n",
        "                    detected_fields[field_type] = extracted_text\n",
        "\n",
        "        return detected_fields\n",
        "\n",
        "    def process_id_image(self, image_path: str) -> Dict[str, str]:\n",
        "        \"\"\"Complete processing pipeline\"\"\"\n",
        "        # Stage 1: Crop ID card\n",
        "        cropped_path, text_extraction_image, final_processed_image, crop_info = self.crop_id_card(image_path)\n",
        "\n",
        "        # Stage 2: Extract national ID using YOLO\n",
        "        digit_detections = self.detect_digits_yolo_only(cropped_path)\n",
        "        national_id, extraction_info = self.construct_national_id_simple(digit_detections)\n",
        "\n",
        "        # Stage 3: Extract other fields\n",
        "        rotation_corrected_path = crop_info.get('rotation_corrected_path')\n",
        "        if rotation_corrected_path:\n",
        "            other_fields = self.detect_other_fields_from_processed_image(text_extraction_image, rotation_corrected_path)\n",
        "        else:\n",
        "            other_fields = self.detect_other_fields_from_processed_image(text_extraction_image, cropped_path)\n",
        "\n",
        "        # Combine results\n",
        "        final_results = {\n",
        "            'national_id': national_id,\n",
        "            **other_fields,\n",
        "            '_extraction_info': extraction_info\n",
        "        }\n",
        "        if self.enable_manual_correction:\n",
        "            final_results = self.manual_correction_interface(final_results)\n",
        "\n",
        "        return final_results\n",
        "\n",
        "    def print_results(self, fields: Dict[str, str]):\n",
        "        \"\"\"Print formatted results\"\"\"\n",
        "        print(\"\\nArabic National ID Detection Results\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        for field_name, value in fields.items():\n",
        "            if field_name.startswith('_'):\n",
        "                continue\n",
        "            print(f\"{field_name.replace('_', ' ').title()}: {value if value else 'Not detected'}\")\n",
        "\n",
        "        if '_extraction_info' in fields:\n",
        "            info = fields['_extraction_info']\n",
        "            print(f\"\\nTechnical Summary:\")\n",
        "            print(f\"Method: {info.get('method', 'unknown')}\")\n",
        "            print(f\"Status: {info.get('status', 'unknown')}\")\n",
        "            print(f\"Digits detected: {info.get('detected_digits', 0)}/{info.get('expected_digits', 14)}\")\n",
        "            print(f\"Average confidence: {info.get('avg_confidence', 0):.2f}\")"
      ],
      "metadata": {
        "id": "am8vAdaqqokt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize detector\n",
        "    detector = SimplifiedArabicIDDetector(\n",
        "        main_model_path=\"/content/best.pt\",\n",
        "        digits_model_path=\"/content/bestd.pt\"\n",
        "    )\n",
        "\n",
        "    # Process image\n",
        "    image_path = \"/content/omda.jpg\"\n",
        "    results = detector.process_id_image(image_path)\n",
        "    detector.print_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIrdsAnmOJnK",
        "outputId": "e2cb02b8-debf-4f44-e133-be019bdb1216"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/omda.jpg: 640x480 1 address1, 1 address2, 1 first_name, 3 ids, 1 last_name, 1 national_id, 9.3ms\n",
            "Speed: 2.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/omda_cropped_id.jpg: 416x640 5 0s, 2 1s, 1 2, 3 3s, 2 4s, 1 7, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/omda_rotation_corrected.jpg: 416x640 1 address1, 1 address2, 1 first_name, 1 id, 1 last_name, 1 national_id, 10.5ms\n",
            "Speed: 2.1ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "============================================================\n",
            "MANUAL CORRECTION MODE\n",
            "============================================================\n",
            "Review and correct any OCR mistakes:\n",
            "Press Enter to keep current value, or type new value to change\n",
            "------------------------------------------------------------\n",
            "\n",
            "National ID:\n",
            "Current: 30307230100414\n",
            "New value (or Enter to keep): \n",
            "✅ Kept original value\n",
            "\n",
            "First Name:\n",
            "Current: محمد\n",
            "New value (or Enter to keep): \n",
            "✅ Kept original value\n",
            "\n",
            "Last Name:\n",
            "Current: عمادالدين محمد عبدالحميد\n",
            "New value (or Enter to keep): \n",
            "✅ Kept original value\n",
            "\n",
            "Address Line 1:\n",
            "Current: ش السعد الخلفاو ى\n",
            "New value (or Enter to keep): \n",
            "✅ Kept original value\n",
            "\n",
            "Address Line 2:\n",
            "Current: الساحل القاهر\n",
            "New value (or Enter to keep): الساحل القاهرة\n",
            "✅ Changed to: الساحل القاهرة\n",
            "\n",
            "============================================================\n",
            "CORRECTION COMPLETED\n",
            "============================================================\n",
            "\n",
            "Arabic National ID Detection Results\n",
            "==================================================\n",
            "National Id: 30307230100414\n",
            "First Name: محمد\n",
            "Last Name: عمادالدين محمد عبدالحميد\n",
            "Address1: ش السعد الخلفاو ى\n",
            "Address2: الساحل القاهرة\n",
            "\n",
            "Technical Summary:\n",
            "Method: yolo_only\n",
            "Status: success\n",
            "Digits detected: 14/14\n",
            "Average confidence: 0.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "23KmlNH2XJST"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}